# DATASETS for NLP/NLG Tasks

[toc]

## List of Propaganda, Misinformation & Hate Speech Datasets

### Ukraine War

* Weibo Dataset: https://blender.cs.illinois.edu/paper/weibo2022.pdf
  * available on github: https://github.com/yrf1/RussiaUkraine_weibo_dataset
* Twitter Dataset: https://arxiv.org/pdf/2203.02955.pdf
  * available on github: https://github.com/ehsanulhaq1/russo_ukraine_dataset
* 2022-Ukraine-Russia-War-Dataset (dataset that describes Equipment Losses & Death Toll & Military  Wounded & Prisoner of War of russians in 2022 Ukraine russia War)
  * available on github: https://github.com/PetroIvaniuk/2022-Ukraine-Russia-War-Dataset

* Ukraine conflict Twitter dataset
  * available on Kaggle: [https://www.kaggle.com/datasets/bwandowando/ukraine-russian-crisis](https://www.kaggle.com/datasets/bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows)


### Misinformation

* Dataset of the study [Not Good Times for Lies: Misinformation Detection on the Russia-Ukraine War, COVID-19, and Refugees](https://arxiv.org/abs/2210.05401)
  * available on github: https://github.com/avaapm/mide22


### Hate Speech

* Hate Speech Dataset Catalogue: https://hatespeechdata.com/

### Propaganda

* The Propaganda Techniques Corpus (PTC)

  *"Propagandistic news articles use specific techniques to convey  their message, such as whataboutism, red Herring, and name calling,  among many others. The Propaganda Techniques Corpus (PTC) allows to  study automatic algorithms to detect them. We provide a permanent  leaderboard to allow researchers both to advertise their progress and to be up-to-speed with the state of the art on the tasks offered (see  below for a definition)."* < see: https://propaganda.qcri.org/ptc/

* Web Interface Demos: https://www.tanbih.org/prta

  1. Propaganda technique analysis on a topic: https://www.tanbih.org/propaganda/

  2. Highlighting of the propaganda techniques in a text: https://www.tanbih.org/propagandasubmit

---

---



## ...some more datasets for NLG Tasks



### Multilingual Datasets

* [COVID-QA](https://huggingface.co/datasets/covid_qa_deepset): Question Answering dataset consisting of 2,019 question/answer pairs annotated by volunteer biomedical experts on scientific articles related to COVID-19
- [Alex Context NLG Dataset](https://github.com/UFAL-DSG/alex_context_nlg_dataset) - A dataset for NLG in dialogue systems in the public transport information domain.
- [Box-score data](https://github.com/harvardnlp/boxscore-data/) - This dataset consists of (human-written) NBA basketball game summaries aligned with their corresponding box- and line-scores.
- [E2E](http://www.macs.hw.ac.uk/InteractionLab/E2E) - This  shared task focuses on recent end-to-end (E2E), data-driven NLG methods, which jointly learn sentence planning and surface realisation from  non-aligned data.
- [Neural-Wikipedian](https://github.com/pvougiou/Neural-Wikipedian) - The repository contains the code along with the required corpora that were used in order to build a system that "learns" how to generate  English biographies for Semantic Web triples.
- [WeatherGov](https://cs.stanford.edu/~pliang/data/weather-data.zip) - Computer-generated weather forecasts from weather.gov (US public forecast), along with corresponding weather data.
- [WebNLG](https://github.com/ThiagoCF05/webnlg) - The enriched version of the WebNLG - a resource for evaluating common NLG tasks,  including Discourse Ordering, Lexicalization and Referring Expression  Generation.
- [WikiBio - wikipedia biography dataset](https://rlebret.github.io/wikipedia-biography-dataset/) - This dataset gathers 728,321 biographies from wikipedia. It aims at evaluating text generation algorithms.
- [The Schema-Guided Dialogue Dataset](https://github.com/google-research-datasets/dstc8-schema-guided-dialogue) - The Schema-Guided Dialogue (SGD) dataset consists of over 20k  annotated multi-domain, task-oriented conversations between a human and a virtual assistant.
- [The Wikipedia company corpus](https://gricad-gitlab.univ-grenoble-alpes.fr/getalp/wikipediacompanycorpus) - Company descriptions collected from Wikipedia. The dataset contains  semantic representations, short, and long descriptions for 51K companies in English.
- [YelpNLG](https://nlds.soe.ucsc.edu/yelpnlg) - YelpNLG provides resources for natural language generation of restaurant reviews.#
- [Trump speeches](https://raw.githubusercontent.com/ryanmcdermott/) - 1mb of text data taken from speeches made by Donald Trump at various points in his 2016 campaign for President of the United States
- [Obama speeches](https://github.com/samim23/obama-rnn)

* [Curated list by NLG tasks](https://aclweb.org/aclwiki/Data_sets_for_NLG)
* [Many free twitter datasets](https://www.trackmyhashtag.com/blog/free-twitter-datasets/)
---



### German Text Datasets

* [GermanQuAD](https://www.deepset.ai/germanquad): human-labeled dataset of 13,722 questions and answers
* [Huge German Corpus (HGC)](https://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/hgc.en.html): A collection of 12.2 million sentences of German newspaper and law  texts. All content has been lemmatized and part-of-speech tagged by  TreeTagger.
* [3 Million German Sentences](https://www.kaggle.com/rtatman/3-million-german-sentences): 3 million German sentences taken from 2015 newspaper texts.  Non-sentences and non-German text has been removed, and information on  word frequency is also included.
* [German Recipes Dataset](https://www.kaggle.com/sterby/german-recipes-dataset): 12,190 German recipes taken from [chefkoch.de](https://www.chefkoch.de/). Each document contains information about ingredients, instructions, creation date and more.
* [German Political Speeches Corpus](http://purl.org/corpus/german-speeches): A collection of 21st century political speeches held by top German  representatives from the German Presidency, Ministry of Foreign Affairs, Chancellery, and Presidency of the Bundestag.
* [NEGRA](http://www.coli.uni-saarland.de/projects/sfb378/negra-corpus/negra-corpus.html): A syntactically annotated corpus of German newspaper texts. Free on  request for all Universities and non-profit organizations. However, you  need to sign and send a form in order to obtain the complete dataset.
* [Digitales Woerterbuch der deutschen Sprache (dlexDB)](http://www.dlexdb.de/): A lexical database for psychological and linguistic research in German. The dataset contains over 100 million German word tokens.
* [Ten Thousand German News Articles Dataset](https://tblock.github.io/10kGNAD/): The first German topic classification dataset. It contains 10,273 German language news articles split up into nine classes.
* [COSMOV â€“ Corpora for Social Movement Research](http://www.cosmov.uzh.ch/) gathers corpora on the language of social movements in the German-speaking world and makes them accessible for online analysis.
* [SUBTLEX-DE](http://crr.ugent.be/subtlex-de/): Word frequencies of 25.4 million words from film and television subtitles.
* https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data.html: Collection of language resources for different NLP research projects. Datasets range from web-scale pre-processed corpora, distributional thesauri, named entity annotation, semantic and lexical substitution, multi-word and complex word annotations to recordings and acoustic models for speech recognition in German. Our datasets are distributed under CC-BY 4.0 license, i.e. free to use for all, whenever possible
* [Metatext List](https://metatext.io/datasets-list/german-language)
---



### (Free) Online Archives

* [Project Gutenberg](www.gutenberg.org)  Library with of over 60,000 free eBook
* [Internet Archive](https://archive.org/) a non-profit  digital library offering free universal access to books, movies &  music, as well as 525 billion archived web pages.
* [UbuWeb](https://ubu.com/) probably the biggest educational resource on the web for avant-garde material (founded by Kenneth Goldsmith)
* [hor.de](https://hor.de/index.html) is a huge collection of public domain German-language poems, notes and word lists

---

#### social movements...

* [Bibliotheks-Verbundkatalog antifaschistischer Archive](http://bibliothek.antifa-archiv.org/) 
* [Verzeichnis Freier Archive, Bibliotheken und Dokumentationsstellen in Deutschland](http://afas-archiv.de/verzeichnis-freier-archive/)
* [Portal der deutschen Umweltbibliotheken](http://www.umweltbibliotheken.de/) 
* [Archiv des Informationszentrums Dritte Welt](https://www.iz3w.org/projekte/das-dritte-welt-archiv)
* [Antifaschistisches Pressearchiv und Bildungszentrum](https://www.apabiz.de/) (apabiz) in Berlin
* [Bibliothek der Freien](https://www.bibliothekderfreien.de/), the biggest anarchist lib.

---

#### open / shadow librairies

- z-lib.org https://z-lib.org/ 
  - Z-Library (z-lib, formerly stylized as BookFinder) is a shadow library and file-sharing project for scholarly journal articles, academic and general-interest books. Z-Library says the project provides access to more than 5.5 million books and over 77.5 million articles
- http://the-eye.eu/ An Open Directory Data Archive
  - The-Eye is a non-profit, community driven platform dedicated to the  archiving and long-term preservation of any and all data including but  by no means limited to...   websites, books, games, software, video, audio, other digital-obscura and ideas.

---



### Allison Parrish's Gutenberg Poetry Corpus

a Gutenberg Poetry corpus, comprised of approximately three million lines of poetry extracted from hundreds of books from [Project Gutenberg](https://gutenberg.org/). The corpus is especially suited to applications in creative computational poetic text generation:

* see: https://github.com/aparrish/gutenberg-poetry-corpus


