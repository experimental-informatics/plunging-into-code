{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7442a139-0304-4431-ad3c-7ccb619940e1",
   "metadata": {},
   "source": [
    "# Markov Chains\n",
    "\n",
    "* Andrei Markov was a Russian mathematician who lived between 1856 and 1922. \n",
    "* He studied mathematics at the university of Petersburg and was lectured by Pafnuty Chebyshev, known for his work in probability theory. \n",
    "* Markov’s first scientific areas were in number theory, convergent series and approximation theory. \n",
    "* His most famous studies were with Markov chains, hence the name and his first paper on the subject was published in 1906. \n",
    "* He was also very interested in poetry and the first application he found of Markov chains was in fact in a linguistic analysis of Pusjkins work [Eugene Onegin](https://en.wikipedia.org/wiki/Eugene_Onegin).\n",
    "\n",
    "Nowaday, the closest example is the text suggestion when we type with our phone, it collects the previous word and make suggestions, based on the probablity from the dataset.\n",
    "\n",
    "[<img src=\"images/typing_suggestion.jpeg\" width=\"250x\"/>](image.png)\n",
    "[Source](https://i.redd.it/1svbyu0mbim21.jpg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bff9b43-caaa-4a94-9477-9d3392fa65db",
   "metadata": {},
   "source": [
    "# <b><u>The statistics of language</u></b>\n",
    "\n",
    "<img src=\"./data/Markov_1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9c813c-4371-4b74-b557-9b2878b35b37",
   "metadata": {},
   "source": [
    "Source: Scientific American, November 1983 < http://bit-player.org/wp-content/extras/bph-publications/SciAm-1983-11-Hayes-drivel.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a0ca6-0543-4188-9312-945a1332a155",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **ZERO ORDER: Randomtext**\n",
    "\n",
    "Sir Arthur Eddington's Statement of 1927: <br>\n",
    "*\"If an army of monkeys were strumming on typewriters, they might write all the books in the British Museum.”*\n",
    "\n",
    "*„The ideal, unbiased monkey would at any moment have an equal probability of striking any key. This behavior can be simulated by a simple strategy. Each symbol in the character set is assigned a number from zero to 27. For each character to be generated a random integer is chosen in the same range and the corresponding character is printed.“*\n",
    "\n",
    "<img src=\"./data/Markov_2.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf0e91-4d77-4129-beb0-303fa804d34d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **FIRST ORDER: Adjust the probability of selecting a letter**.\n",
    "\n",
    "Important! Of course, frequencies for individual words can also be used as a basis.\n",
    "\n",
    "<img src=\"./data/Markov_3.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f95d0-f06b-4e41-a34b-ec443ef30f02",
   "metadata": {},
   "source": [
    "\n",
    "FIRST ORDER text for letter frequencies in James Joyce's Ulysses.\n",
    "\n",
    "<img src=\"./data/Markov_4.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc15cb-5756-4b64-a032-c651630c59f0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **SECOND ORDER: Adjust the probability of selecting a letter**.\n",
    "\n",
    "“The next refinement is a crucial one because it can be extended, at least in principle, to an arbitrarily high order.\n",
    "\n",
    "“The root of the idea is that a letter‘s probability of appearing at a given point in written English depends strongly on the preceding letters. After a v, for example, an e is most likely; after a q, a u is all but certain. The procedure, then, is to set up a separate frequency table for each symbol in the character set.”\n",
    "\n",
    "**Statistical consideration of the context of a letter!**\n",
    "\n",
    "<img src=\"./data/Markov_6.jpg\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98cbc58-1d18-4529-a7f8-55c069fce837",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./data/Markov_5.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07146fc0-b23c-461c-a03e-75f174c71c49",
   "metadata": {},
   "source": [
    "**Important!** Of course, frequencies for individual words can also be used as a basis.\n",
    "\n",
    "<img src=\"./data/Markov_8.png\">\n",
    "\n",
    "[Source](https://mb-14.github.io/tech/2018/10/24/gomarkov.html)\n",
    "\n",
    "All information used to compute the next value lies in the current value. So the next value is not based on a sequence of previous values, but just on the one current value.\n",
    "\n",
    "Transition from one state to another are according to certain probabilistic rules.\n",
    "\n",
    "#### Example \n",
    "\n",
    "As we can see from the image, it's is a trained Markov-chain. if we start from the word `I`, we can see that there is a 2/3 chance the move into `am` and 1/3 chance to move to `like`. \n",
    "\n",
    "If we choose the highest probablity(which is `am`), the next possible token will be `an` or `sam`. we can then pick it randomly. \n",
    "\n",
    "In the end we could get the sentence ` I am an engineer` or `I am Sam`\n",
    "\n",
    "\n",
    "#### Procedure\n",
    "\n",
    "These are the procedure we need to go through to work with Markov-Chain\n",
    "\n",
    "- Input and clean the training text.\n",
    "\n",
    "- Create a list of all words in a given text. \n",
    "\n",
    "- Find a possible following word, and create a dictionary with it. (As the graph above).\n",
    "\n",
    "- if a word appears as input: lookup the word in the dictionary and choose one of the options.\n",
    "\n",
    "- Repeat the last procedure to generate infinite text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7cbb14-b073-4b3d-8c65-04528d5fed48",
   "metadata": {},
   "source": [
    "## **THIRD ORDER:**\n",
    "\n",
    "“The next step should be obvious. A third-order algorithm chooses each letter in the random text according to probabilities determined by the two preceding letters. \n",
    "\n",
    "This calls for a three-dimensional array with 28 planes, each plane being made up of 28 rows of 28 columns.”\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1364b74-e71f-438c-9ebf-0e1f5d4c82d2",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./data/Markov_7.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072648c3-42a9-4a45-8eb7-2bde7574d606",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "# Markov Chains & Google's PageRank\n",
    "\n",
    "[The PageRank Citation Ranking: Bringing Order to the Web](http://ilpubs.stanford.edu:8090/422/) < by Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: Bringing order to the web. Technical Report 1999-66, Stanford InfoLab, November 1999.\n",
    "\n",
    "additional sources:\n",
    "* https://medium.com/analytics-vidhya/google-page-rank-and-markov-chains-d65717b98f9c\n",
    "* https://www2.math.upenn.edu/~kazdan/312F12/JJ/MarkovChains/markov_google.pdf\n",
    "* https://disco.ethz.ch/courses/fs16/ti2/lecture/chapter11.pdf\n",
    "\n",
    "---\n",
    "\n",
    "The PageRank algorithm is developed by Google founders Larry Page and Sergey Brin in 1996. Google has further developed this method. The current model details were kept confidential but still the current method is based on the original one.\n",
    "\n",
    "PageRank algorithm became the Heart of the Google Search Engine!\n",
    "\n",
    "**Brin and Page considered web surfing as a stochastic process:** < *Markov processes are stochastic processes, traditionally in discrete or continuous time, that have the Markov property, which means the next value of the Markov process depends on the current value, but it is conditionally independent of the previous values of the stochastic process. In other words, the behavior of the process in the future is stochastically independent of its behavior in the past, given the current state of the process.*\n",
    "\n",
    "PageRank can be thought of as a model of user behavior. We assume there is a “random surfer” who is given a web page at random and keeps clicking on links, never hitting “back” but eventually gets bored and starts on another random page.\n",
    "- i.e., surfer clicks on a link on the current page with probability 0.85; opens up a random page with probability 0.15.\n",
    "- A page’s rank is the probability the random user will end up on that page, OR, equivalently\n",
    "- the fraction of time the random user spends on that page in the long run.\n",
    "\n",
    "![](./images/pagerank1.jpg)\n",
    "\n",
    "*Page rank is computed based on the incoming and outgoing links to the page. Link from a high reputed page has a higher weight compared to that of the link from a low reputed page. So more the incoming links a page has, more important the page is. (Image Credits: Wikimedia)*\n",
    "\n",
    "---\n",
    "\n",
    "It’s a popular link-based ranking algorithm. Rather than going into the content and ranking the pages, Page rank makes use of the linked structure to rank the pages.\n",
    "\n",
    "World Wide Web(WWW) is represented as a directed graph W in which all the nodes are pages and edges are hyperlinks. This directed graph is called a Web graph.\n",
    "\n",
    "![](./images/pagerank2.jpg)\n",
    "\n",
    "*A small network with 4 web pages*\n",
    "\n",
    "    \n",
    "    Google’s idea was to model a random surfer who follows hyperlinks in the web graph, i.e., performs a random walk. After sufficiently many steps, the websites can be ranked by how many times they were visited. The intuition is that websites are visited more often if they are linked by many other sites, which should be a good measure of how important a website is.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
